# CoSMo 2017 Project
* It is important to explain why simple ideas don't work, just as it is important to explain why our complex model works
  * So, it is always a good idea to get a tree going for all the proposed models and explicitly say why each of them failed.

## *FSM* section
*

## *Bayesian Generalization* section
* What is the concept of 'explaining way' in the context of our bleed-off of learning into other angles.
  * This paper explains explaining away: http://strategicreasoning.org/wp-content/uploads/2010/03/pami93.pdf
  * Udacity's intro to AI talks about explaining away as well.
  * More web resources here:
    * https://stats.stackexchange.com/questions/54849/why-does-explaining-away-make-intuitive-sense
    * https://www.eecs.qmul.ac.uk/~norman/BBNs/The_notion_of__explaining_away__evidence.htm
    * https://www.eecs.qmul.ac.uk/~norman/BBNs/Analysing_a_BBN__entering_evidence_and_propagation.htm
* The ideas of sharpening of the estimate in one angle widening the curve for the estimate of another angle might be based on another concept called lateral inhibition.
* Feedback is often used as RL rules.
